
<html xmlns=http://www.w3.org/1999/xhtml>
<head>
  <title>Jonne Sälevä</title>
  <style type=text/css>code{white-space: pre;}</style>
  <link rel=stylesheet href=../static/css/style.css type=text/css />
  <script src=https://polyfill.io/v3/polyfill.min.js?features=es6></script>
  <script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
</head>

<body>
<h1 id="yelp-review-sentiment-analysis">yelp review sentiment analysis</h1> </div> <thead> <tr> <td> <a href="/">Home</a> </td> <td> <a href="/research">Research</a> </td> <td> <a href="/portfolio">Portfolio</a> </td> <td> <a href="/resume">Resume</a> </td> </tr> </thead> <hr /> <p><em>Quick note from Jonne, march 2020: This page is a HTML version of my writeup for this project.</em></p> <p><em>Overall, the goal was to gauge the sentiment of yelp reviews as being positive or negative, using a maximum entropy classifier and stochastic gradient descent optimizer implemented from scratch. I enjoyed this project a lot and had a good time implementing things with only numpy.</em></p> <p><em>For more information about the files referenced in the writeup below, see the <a href="https://gitlab.com/jonnesaleva/maxent-sgd-from-scratch">project repository</a> on Gitlab.</em></p> <hr /> <h3 id="code-structure">Code Structure</h3> <p>The code is centered around <code>maxent.py</code> which contains the logic to fit the MaxEnt model and classify examples. The experiments are located in <code>exp1_exp2_grid.py</code> as well as <code>exp3.py</code>. Feature extraction is done in <code>extract_features.py</code> and leverages the <code>sklearn</code> library. To run the code, see instructions in <code>README.md</code>. The instructions have been tested in a clean Python 3.7 environment with Miniconda. After running the code, the results of individual experiments will be saved in the <code>results</code> folder.</p> <h3 id="experimental-settings">Experimental Settings</h3> <p>Our model is trained on data <span class="math inline">\(\mathbb{D} = \{(\mathbf{\phi(x)}_i, y_i)\}_{i=1}^n\)</span> where each observation consists of a vector of “feature counts” <span class="math inline">\(\mathbf{\phi(x)} \in \mathbb{R}^D\)</span> and a label <span class="math inline">\(y\)</span>, represented either as an integer or a one-hot vector <span class="math inline">\(\mathbf{y} \in \{0,1\}^K\)</span>, where <span class="math inline">\(K\)</span> is the number of classes.</p> <p>We transform <span class="math inline">\(\mathbf{\phi}(x)\)</span> into a K-dimensional probability vector <span class="math inline">\(\mathbf{\pi}\)</span> via the <span class="math inline">\(\text{softmax}\)</span> function and a dot product between the feature count vector and class-specific parameter vector <span class="math inline">\(\theta_k\)</span>: <span class="math inline">\(P(y_i = k) = \pi_k(\mathbf{\phi(x)}_i) = \frac{\text{exp}(\mathbf{\theta_k} \cdot \mathbf{\phi}(x)_i)}{\sum_{j=1}^K \text{exp}(\mathbf{\theta_j} \cdot \mathbf{\phi}(x_i))}\)</span></p> <p>The “feature counts” in our case correspond to a “bag of words” counts, so the dimensionality <span class="math inline">\(D\)</span> is in fact equal to the size of the vocabulary, <span class="math inline">\(|V|\)</span>. We stack the class-specific parameter vectors into a parameter matrix <span class="math inline">\(\Theta \in \mathbb{R}^{K \times D}\)</span>.</p> <p>The loss function, ie. the averaged negative log likelihood, is computed in matrix form as <span class="math inline">\(\lambda ||\Theta||_F^2 - \frac{1}{N}\mathbf{1}^\top(\mathbf{Y}\otimes\log\mathbf{\Pi})\mathbf{1} = \lambda ||\Theta||_F^2 - \frac{1}{N} \sum_{i=1}^N \sum_{k=1}^K \mathbf{1}\{y_i = k\} \log P(y_i = k)\)</span> where <span class="math inline">\(\otimes\)</span> denotes the elementwise matrix product. <span class="math inline">\(\mathbf{Y}\)</span> and <span class="math inline">\(\mathbf{\Pi}\)</span> are the K-by-N matrices of observed labels and softmax probabilities.</p> <p>The gradient is computed as <span class="math inline">\(\nabla L = \lambda\Theta - \frac{1}{M} \sum_i (\mathbf{1}\{y_i\} - \pi(\phi(x_i)))\odot\Phi_i\)</span> where <span class="math inline">\(\Phi_i\)</span> is K-by-D matrix consisting of stacking the feature vector <span class="math inline">\(\phi(x_i)\)</span> on top of itself K times and <span class="math inline">\(\odot\)</span> denotes the row-wise vector-matrix product. <span class="math inline">\(M\)</span> denotes the minibatch size.</p> <h3 id="experiments-1-2">Experiments 1 &amp; 2</h3> <p>We run these together as a grid and work through the various values for the training set size <span class="math inline">\(N\)</span> and minibatch size <span class="math inline">\(M\)</span>.</p> <center> <figure> <img src="../static/img/training_set_size_vs_accuracy.png" style="height:20.0%" alt="" /><figcaption>Training set size vs accuracy</figcaption> </figure> <figure> <img src="../static/img/minibatch_size_vs_accuracy.png" style="height:20.0%" alt="" /><figcaption>Minibatch size vs accuracy</figcaption> </figure> <figure> <img src="../static/img/heatmap_l2_combined.png" alt="" /><figcaption>Heatmap &amp; L2 penalty vs accuracy</figcaption> </figure> </center> <p>Overall, it seems like as long as the minibatch is not too small, and we take a large enough set as our training set (say, 1000 to 10000 samples), we gain all the accuracy we can, at least as measured on the development set. Beyond that, larger minibatches simply cause a lot of computational overhead without much gain. It would most likely have been useful to run the experiments several times and plot several curves for each experimental condition, in order to get an idea of the variance in the outcome. Based on the heatmap on the last page, we adopt <span class="math inline">\(N=50000\)</span> and <span class="math inline">\(M=100\)</span> as our settings for training set size and minibatch size for Experiment 3.</p> <h3 id="experiment-3">Experiment 3</h3> <p>For experiment 3, we simply note that increasing the regularization penalty only seems to harm our results. In fact, this seemed rather surprising, given the large dimensionality of our features. In <code>maxent_test.py</code>, we only apply <span class="math inline">\(\lambda = 0.01\)</span> as our regularization penalty for this reason.</p> <hr /> <tfoot> <tr> <td> © Jonne Sälevä, 2020 </td> </tr> </tfoot>

</body>
</html>

