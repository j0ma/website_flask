
<html xmlns=http://www.w3.org/1999/xhtml>
<head>
  <title>Jonne Sälevä</title>
  <style type=text/css>code{white-space: pre;}</style>
  <link rel=stylesheet href=../static/css/style.css type=text/css />
  <script src=https://polyfill.io/v3/polyfill.min.js?features=es6></script>
  <script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
</head>

<body>
<h1 id="discourse-relation-classification">discourse relation classification</h1> </div> <thead> <tr> <td> <a href="/">Home</a> </td> <td> <a href="/research">Research</a> </td> <td> <a href="/portfolio">Portfolio</a> </td> <td> <a href="/resume">Resume</a> </td> </tr> </thead> <hr /> <p><em>Quick note from Jonne, march 2020: This page is a HTML version of my writeup for this project.</em></p> <p><em>Overall, the goal was to classify text into one of several different discourse relation classes, and the task was based on the <a href="https://www.cs.brandeis.edu/~clp/conll16st/">CoNLL 2016 Shallow Discourse Parsing Task</a>. </em></p> <p><em>For more information about the files referenced in the writeup below, see the <a href="https://gitlab.com/jonnesaleva/discourse-relation-classification">project repository</a> on Gitlab.</em></p> <hr /> <h3 id="code-structure">Code Structure</h3> <p>Overall, the main workhorse of the code is the <code>experiment.py</code> file which contains all classes required to run experiments. To run the code, follow instructions in <code>README.md</code>. Everything should be properly automated, and can be executed using various <code>make</code> commands. The experiments are run using <code>run_ff.py</code> and <code>run_cnn.py</code> which instantiate several <code>Experiment</code>s corresponding to the various hyperparameters that get executed.</p> <p>In addition to the <code>.py</code> files there are also two Jupyter Notebooks, containing <em>extensive</em> visualizations and tabular analyses. We wholeheartedly encourage the grader to peek at those in order to get a full idea beyond the 3 pages of this report.</p> <h3 id="experimental-settings">Experimental Settings</h3> <h4 id="feature-representations-and-other-design-desicions">Feature representations and other design desicions</h4> <p>For word vectors, we opted to use 300-dimensional <code>FastText</code> vectors since they are trained to handle subword representations. To further avoid OOV issues, we vectorized all my sentences using <code>pymagnitude</code> package, which auto-handles subword representations, so there was no need to perform lemmatization or stemming of any kind.</p> <p>To construct the feature representations, we first pick 25 tokens from Arg1 and Arg2, but in such a way that Arg1 tokens get extracted in reverse order, so that the argument tokens form a “window” around the connective. This was an approach suggested by <code>&lt;REDACTED&gt;</code>. For the connective, we simply pick 2 tokens. These numbers are justified by looking at the empirical distribution of token counts in Arg1, Arg2 and the connective, and picking values that cover over 90% of the input sentences. This analysis can be reproduced using <code>make explore</code> after feature extraction has been performed.</p> <p>In line with what was discussed by the teaching staff, we first stack the vectors of tokens in a sentence together verticall, forming a N-b-300 representation. We call this condition the <code>stacked</code> condition. In addition, we experiment with <strong>averaging</strong> the rows of said matrix, to obtain a <strong>single</strong> 300 dimensional vector for the sentence. Two averaging conditions are introduced: either we average the word vectors for the entire sentence, or simpl the 52 tokens discussed previously. These form the <code>average_entire</code> and <code>average_padded</code> conditions.</p> <h4 id="hyperparameter-configurations">Hyperparameter configurations</h4> <p>For feedforward networks, we first project the input to 512 hidden units (arbitrarily), and then follow with several layers according to three “layer conditions”, or shapes: <code>funnel</code>, <code>bottleneck</code> and <code>double bottleneck</code>. In the <code>funnel</code> condition the layer sizes simply decrease, e.g. 512, 128, 64. In <code>bottleneck</code>, there is a small layer in between two larger ones, e.g. 512, 256, 64, 128. In <code>double bottleneck</code> we simply repeat a bottleneck twice. Finall, there is a softmax layer of 21 units. We also run feedforward networks using all three feature representations, <code>stacked</code>, <code>average_entire</code> and <code>average_padded</code>. Further, we experiment with whether including dropout after each full connected layer helps or not. In case dropout is used, its probability is kept constant at 0.5.</p> <p>For convolutional networks, we need an input matrix so we only use the <code>stacked</code> condition. In terms of parameters, we vary the number of convolutional layers, filters used, the kernel size, the inclusion of dropout after convolutional layers (fully connected layers always have dropout).</p> <h3 id="experiments">Experiments</h3> <p>For both feedforward and convolutional networks, we run a grid of all hyperparameters for 50 epochs. Overall, this probably took around 12-15 hours to run. There are a total of 78 experiments for feedforward and 54 for convolutional networks.</p> <h3 id="results-discussion">Results &amp; discussion</h3> <p>After running the experiments, it is fairly clear that we could have managed with running the training for fewer epochs. Looking at the validation accuracy vs epochs, it is obvious that by 20 epochs, everything has already plateaued.</p> <center> <figure> <img src="../static/img/val_acc_vs_epoch.png" alt="Validation accuracy vs epoch" height="256" /><figcaption>Validation accuracy vs epoch</figcaption> </figure> </center> <p>In terms of what conditions outperform others, there are a few points to make but the results are not entirely clear. Features in the <code>stacked</code> condition do seem to be top performers for feedforward networks, with only stacked features present in the top 10 conditions.</p> <p>Dropout also seems to help a great deal for feedforward networks, yielding on average about 0.1 absolute improvement in accuracy. This can as can be seen from the picture below:</p> <center> <figure> <img src="../static/img/dropout_validation_treatment_effect_feedforward.png" alt="Effect of dropout on validation accuracy" height="256" /><figcaption>Effect of dropout on validation accuracy</figcaption> </figure> </center> <p>However, for convolutional networks it is not the case; in fact it seems to hurt performance.</p> <center> <figure> <img src="../static/img/dropout_validation_treatment_effect_val_conv.png" alt="Effect of convolutional dropout on validation accuracy" height="256" /><figcaption>Effect of convolutional dropout on validation accuracy</figcaption> </figure> </center> <p>Across all conditions, the best accuracy we see on the validation set is in the high 80s.</p> <h4 id="test-set">Test set</h4> <p>Overall the out-of-domain test scores are much lower. The best values for F1 seem to be around 0.5. Recall is definitely the weakest metric, especially in implicit cases</p> <p>Overall we can see that the evaluation metrics produce similar distributions for convolutional networks and feedforward ones, as long as <code>stacked</code> features are used:</p> <center> <figure> <img src="../static/img/evaluation_metrics_feedforward_stacked.png" alt="Evaluation metric distribution for feedforward networks" height="256" /><figcaption>Evaluation metric distribution for feedforward networks</figcaption> </figure> <figure> <img src="../static/img/evaluation_metrics_convolutional.png" alt="Evaluation metric distribution for convolutional networks" /><figcaption>Evaluation metric distribution for convolutional networks</figcaption> </figure> </center> <p>Therefore, overall there is no clear conclusion to things. Dropout helps, stacking helps, but personally I would still opt for feedforward over convolutional networks, given the ease of implementation.</p> <p>Please see the <a href="https://gitlab.com/jonnesaleva/discourse-relation-classification">Gitlab repository</a> for more analysis!</p> <hr /> <tfoot> <tr> <td> © Jonne Sälevä, 2020 </td> </tr> </tfoot>

</body>
</html>

